ENCODER_MODEL_MAPPING = {
    # CLIP Models
    "CLIP Base (LAION, Patch 16)": "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
    "CLIP Base (OpenAI, Patch 16)": "openai/clip-vit-base-patch16",
    "CLIP Base (OpenAI, Patch 32)": "openai/clip-vit-base-patch32",
    # CLIP Large Models
    "CLIP Large (OpenAI, Patch 14)": "openai/clip-vit-large-patch14",
    "CLIP Large HQ (OpenAI, 336px)": "openai/clip-vit-large-patch14-336",
    # CLIP BigG Models
    "CLIP BigG-14 (LAION, Patch 14)": "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k",
    # T5 Text Models
    "T5 Small (Google)": "google-t5/t5-small",
    "T5 XXL v1.1 (Google)": "google/t5-v1_1-xxl",
}
